2022-03-30 09:07:22,508	INFO	torchdistill.common.main_util	Not using distributed mode
2022-03-30 09:07:22,508	INFO	__main__	Namespace(adjust_lr=False, config='configs/sample/cifar10/kd/resnet18_from_resnet50_fkd.yaml', device='cuda', dist_url='env://', log='log/cifar10/kd/resnet18_from_resnet50_fkl_2.txt', log_config=False, seed=None, start_epoch=0, student_only=False, test_only=False, world_size=1)
2022-03-30 09:07:22,517	INFO	torchdistill.datasets.util	Loading train data
2022-03-30 09:07:23,020	INFO	torchdistill.datasets.util	dataset_id `cifar10/train`: 0.5030937194824219 sec
2022-03-30 09:07:23,020	INFO	torchdistill.datasets.util	Loading val data
2022-03-30 09:07:23,432	INFO	torchdistill.datasets.util	dataset_id `cifar10/val`: 0.4112999439239502 sec
2022-03-30 09:07:23,432	INFO	torchdistill.datasets.util	Loading test data
2022-03-30 09:07:23,845	INFO	torchdistill.datasets.util	dataset_id `cifar10/test`: 0.4134547710418701 sec
2022-03-30 09:07:23,895	INFO	torchdistill.common.main_util	ckpt file is not found at `./resource/ckpt/cifar10/teacher/cifar10-resnet56.pt`
2022-03-30 09:07:25,950	INFO	torchdistill.common.main_util	Loading model parameters
2022-03-30 09:07:25,954	INFO	__main__	Start training
2022-03-30 09:07:25,955	INFO	torchdistill.models.util	[teacher model]
2022-03-30 09:07:25,955	INFO	torchdistill.models.util	Using the original teacher model
2022-03-30 09:07:25,955	INFO	torchdistill.models.util	[student model]
2022-03-30 09:07:25,955	INFO	torchdistill.models.util	Using the original student model
2022-03-30 09:07:25,955	INFO	torchdistill.core.distillation	Loss = 1.0 * OrgLoss + 1 * KFLoss(
  (adaptivepool2d): AdaptiveAvgPool2d(output_size=(1, 1))
  (flatten): Flatten(start_dim=1, end_dim=-1)
)
2022-03-30 09:07:25,955	INFO	torchdistill.core.distillation	Freezing the whole teacher model
2022-03-30 09:07:25,961	INFO	torchdistill.common.main_util	Loading optimizer parameters
2022-03-30 09:07:25,963	INFO	torchdistill.common.main_util	Loading scheduler parameters
2022-03-30 09:07:27,085	INFO	torchdistill.misc.log	Epoch: [0]  [  0/391]  eta: 0:07:18  lr: 0.0010000000000000002  img/s: 224.92015383713039  loss: 0.0248 (0.0248)  time: 1.1211  data: 0.5519  max mem: 286
2022-03-30 09:07:28,748	INFO	torchdistill.misc.log	Epoch: [0]  [100/391]  eta: 0:00:08  lr: 0.0010000000000000002  img/s: 8213.179616626127  loss: 0.0272 (0.0291)  time: 0.0163  data: 0.0001  max mem: 286
2022-03-30 09:07:30,356	INFO	torchdistill.misc.log	Epoch: [0]  [200/391]  eta: 0:00:04  lr: 0.0010000000000000002  img/s: 8055.440034810269  loss: 0.0255 (0.0292)  time: 0.0158  data: 0.0001  max mem: 286
2022-03-30 09:07:31,974	INFO	torchdistill.misc.log	Epoch: [0]  [300/391]  eta: 0:00:01  lr: 0.0010000000000000002  img/s: 7909.700360957643  loss: 0.0260 (0.0299)  time: 0.0159  data: 0.0001  max mem: 286
2022-03-30 09:07:33,540	INFO	torchdistill.misc.log	Epoch: [0] Total time: 0:00:07
2022-03-30 09:07:34,123	INFO	torchdistill.misc.log	Validation:  [ 0/79]  eta: 0:00:45  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 0.5809  data: 0.5747  max mem: 286
2022-03-30 09:07:34,435	INFO	torchdistill.misc.log	Validation: Total time: 0:00:00
2022-03-30 09:07:34,435	INFO	__main__	 * Acc@1 92.9900	Acc@5 99.8300

2022-03-30 09:07:35,057	INFO	torchdistill.misc.log	Epoch: [1]  [  0/391]  eta: 0:04:02  lr: 0.0010000000000000002  img/s: 4491.403311219496  loss: 0.0468 (0.0468)  time: 0.6213  data: 0.5925  max mem: 286
2022-03-30 09:07:36,735	INFO	torchdistill.misc.log	Epoch: [1]  [100/391]  eta: 0:00:06  lr: 0.0010000000000000002  img/s: 7841.309127024698  loss: 0.0267 (0.0322)  time: 0.0161  data: 0.0001  max mem: 286
2022-03-30 09:07:38,357	INFO	torchdistill.misc.log	Epoch: [1]  [200/391]  eta: 0:00:03  lr: 0.0010000000000000002  img/s: 7959.421090865962  loss: 0.0264 (0.0313)  time: 0.0161  data: 0.0001  max mem: 286
2022-03-30 09:07:39,961	INFO	torchdistill.misc.log	Epoch: [1]  [300/391]  eta: 0:00:01  lr: 0.0010000000000000002  img/s: 8390.0500398506  loss: 0.0299 (0.0308)  time: 0.0164  data: 0.0001  max mem: 286
2022-03-30 09:07:41,468	INFO	torchdistill.misc.log	Epoch: [1] Total time: 0:00:07
2022-03-30 09:07:42,051	INFO	torchdistill.misc.log	Validation:  [ 0/79]  eta: 0:00:45  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.5804  data: 0.5743  max mem: 286
2022-03-30 09:07:42,371	INFO	torchdistill.misc.log	Validation: Total time: 0:00:00
2022-03-30 09:07:42,371	INFO	__main__	 * Acc@1 92.9400	Acc@5 99.7900

2022-03-30 09:07:42,998	INFO	torchdistill.misc.log	Epoch: [2]  [  0/391]  eta: 0:04:04  lr: 0.0010000000000000002  img/s: 4516.416215897906  loss: 0.0319 (0.0319)  time: 0.6257  data: 0.5973  max mem: 286
2022-03-30 09:07:44,678	INFO	torchdistill.misc.log	Epoch: [2]  [100/391]  eta: 0:00:06  lr: 0.0010000000000000002  img/s: 8257.773894853417  loss: 0.0292 (0.0308)  time: 0.0165  data: 0.0001  max mem: 286
2022-03-30 09:07:46,347	INFO	torchdistill.misc.log	Epoch: [2]  [200/391]  eta: 0:00:03  lr: 0.0010000000000000002  img/s: 7954.585906477805  loss: 0.0281 (0.0312)  time: 0.0166  data: 0.0001  max mem: 286
2022-03-30 09:07:47,991	INFO	torchdistill.misc.log	Epoch: [2]  [300/391]  eta: 0:00:01  lr: 0.0010000000000000002  img/s: 8296.311534182223  loss: 0.0271 (0.0307)  time: 0.0160  data: 0.0001  max mem: 286
2022-03-30 09:07:49,496	INFO	torchdistill.misc.log	Epoch: [2] Total time: 0:00:07
2022-03-30 09:07:50,086	INFO	torchdistill.misc.log	Validation:  [ 0/79]  eta: 0:00:46  acc1: 95.3125 (95.3125)  acc5: 100.0000 (100.0000)  time: 0.5877  data: 0.5830  max mem: 286
2022-03-30 09:07:50,396	INFO	torchdistill.misc.log	Validation: Total time: 0:00:00
2022-03-30 09:07:50,396	INFO	__main__	 * Acc@1 92.8800	Acc@5 99.8100

2022-03-30 09:07:51,023	INFO	torchdistill.misc.log	Epoch: [3]  [  0/391]  eta: 0:04:04  lr: 0.0010000000000000002  img/s: 5447.203319838878  loss: 0.0304 (0.0304)  time: 0.6259  data: 0.6023  max mem: 286
2022-03-30 09:07:52,621	INFO	torchdistill.misc.log	Epoch: [3]  [100/391]  eta: 0:00:06  lr: 0.0010000000000000002  img/s: 8245.471763603693  loss: 0.0298 (0.0302)  time: 0.0160  data: 0.0001  max mem: 286
2022-03-30 09:07:54,265	INFO	torchdistill.misc.log	Epoch: [3]  [200/391]  eta: 0:00:03  lr: 0.0010000000000000002  img/s: 8147.493125322488  loss: 0.0245 (0.0305)  time: 0.0170  data: 0.0001  max mem: 286
2022-03-30 09:07:55,928	INFO	torchdistill.misc.log	Epoch: [3]  [300/391]  eta: 0:00:01  lr: 0.0010000000000000002  img/s: 8365.472240833944  loss: 0.0252 (0.0301)  time: 0.0158  data: 0.0001  max mem: 286
2022-03-30 09:07:57,444	INFO	torchdistill.misc.log	Epoch: [3] Total time: 0:00:07
2022-03-30 09:07:58,030	INFO	torchdistill.misc.log	Validation:  [ 0/79]  eta: 0:00:46  acc1: 94.5312 (94.5312)  acc5: 100.0000 (100.0000)  time: 0.5842  data: 0.5796  max mem: 286
2022-03-30 09:07:58,323	INFO	torchdistill.misc.log	Validation: Total time: 0:00:00
2022-03-30 09:07:58,323	INFO	__main__	 * Acc@1 92.9800	Acc@5 99.8300

2022-03-30 09:07:58,950	INFO	torchdistill.misc.log	Epoch: [4]  [  0/391]  eta: 0:04:04  lr: 0.0010000000000000002  img/s: 4066.0641486855957  loss: 0.0480 (0.0480)  time: 0.6252  data: 0.5936  max mem: 286
2022-03-30 09:08:00,601	INFO	torchdistill.misc.log	Epoch: [4]  [100/391]  eta: 0:00:06  lr: 0.0010000000000000002  img/s: 7961.42764777412  loss: 0.0255 (0.0323)  time: 0.0171  data: 0.0001  max mem: 286
2022-03-30 09:08:02,319	INFO	torchdistill.misc.log	Epoch: [4]  [200/391]  eta: 0:00:03  lr: 0.0010000000000000002  img/s: 5841.077018484872  loss: 0.0264 (0.0312)  time: 0.0167  data: 0.0001  max mem: 286
