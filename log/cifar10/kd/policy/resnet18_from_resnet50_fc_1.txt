2022-04-17 23:27:03,298	INFO	torchdistill.common.main_util	Not using distributed mode
2022-04-17 23:27:03,298	INFO	__main__	Namespace(adjust_lr=False, config='configs/sample/cifar10/kd/resnet18_from_resnet50_policy.yaml', device='cuda', dist_url='env://', log='log/cifar10/kd/policy/resnet18_from_resnet50_fc_1.txt', log_config=False, seed=None, start_epoch=0, student_only=False, test_only=False, world_size=1)
2022-04-17 23:27:03,312	INFO	torchdistill.datasets.util	Loading train data
2022-04-17 23:27:03,840	INFO	torchdistill.datasets.util	dataset_id `cifar10/train`: 0.5276579856872559 sec
2022-04-17 23:27:03,840	INFO	torchdistill.datasets.util	Loading val data
2022-04-17 23:27:04,252	INFO	torchdistill.datasets.util	dataset_id `cifar10/val`: 0.4117910861968994 sec
2022-04-17 23:27:04,252	INFO	torchdistill.datasets.util	Loading test data
2022-04-17 23:27:04,664	INFO	torchdistill.datasets.util	dataset_id `cifar10/test`: 0.41176724433898926 sec
2022-04-17 23:27:04,707	INFO	torchdistill.common.main_util	ckpt file is not found at `./resource/ckpt/cifar10/kd/cifar10-resnet56-origin-o.pt`
2022-04-17 23:27:07,320	INFO	__main__	Start training
2022-04-17 23:27:07,323	INFO	torchdistill.models.util	[teacher model]
2022-04-17 23:27:07,323	INFO	torchdistill.models.util	Using the WrapperPolicy teacher model
2022-04-17 23:27:07,323	INFO	torchdistill.models.util	Frozen module(s): {'model'}
2022-04-17 23:27:07,323	INFO	torchdistill.models.util	[student model]
2022-04-17 23:27:07,323	INFO	torchdistill.models.util	Using the EmptyModule student model
2022-04-17 23:27:07,324	INFO	torchdistill.core.distillation	Loss = 1.0 * AuxPolicyKDLoss(
  (linear): Linear(in_features=128, out_features=16, bias=True)
  (mse): MSELoss()
)
2022-04-17 23:27:07,324	INFO	torchdistill.core.distillation	Freezing the whole student model
2022-04-17 23:27:07,325	INFO	torchdistill.core.distillation	Note that you are training some/all of the modules in the teacher model
2022-04-17 23:27:07,325	INFO	torchdistill.core.distillation	Started stage 1
2022-04-17 23:27:08,079	INFO	torchdistill.misc.log	Epoch: [0]  [  0/782]  eta: 0:09:48  lr: 0.1  img/s: 761.8388883843429  loss: 7.2877 (7.2877)  time: 0.7525  data: 0.6684  max mem: 101
2022-04-17 23:27:08,878	INFO	torchdistill.misc.log	Epoch: [0]  [100/782]  eta: 0:00:10  lr: 0.1  img/s: 9921.842764738496  loss: 3.7384 (3.8879)  time: 0.0076  data: 0.0001  max mem: 101
