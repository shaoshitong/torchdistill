2022-04-18 14:49:09,327	INFO	torchdistill.common.main_util	Not using distributed mode
2022-04-18 14:49:09,327	INFO	__main__	Namespace(adjust_lr=False, config='configs/sample/cifar10/kd/resnet18_from_resnet50_policy_stage2.yaml', device='cuda', dist_url='env://', ema=True, log='log/cifar10/kd/policy/resnet18_from_resnet50_1.0_0.9_0.5_load_freeze_student.txt', log_config=False, seed=None, start_epoch=0, student_only=False, test_only=False, world_size=1)
2022-04-18 14:49:09,337	INFO	torchdistill.datasets.util	Loading train data
2022-04-18 14:49:09,868	INFO	torchdistill.datasets.util	dataset_id `cifar10/train`: 0.5309348106384277 sec
2022-04-18 14:49:09,868	INFO	torchdistill.datasets.util	Loading val data
2022-04-18 14:49:10,282	INFO	torchdistill.datasets.util	dataset_id `cifar10/val`: 0.41387081146240234 sec
2022-04-18 14:49:10,282	INFO	torchdistill.datasets.util	Loading test data
2022-04-18 14:49:10,693	INFO	torchdistill.datasets.util	dataset_id `cifar10/test`: 0.41092586517333984 sec
2022-04-18 14:49:10,740	INFO	torchdistill.common.main_util	ckpt file is not found at `./resource/ckpt/cifar10/kd/cifar10-resnet56-origin-o.pt`
2022-04-18 14:49:13,302	INFO	__main__	Start training
2022-04-18 14:49:13,305	INFO	torchdistill.models.util	[teacher model]
2022-04-18 14:49:13,305	INFO	torchdistill.models.util	Using the WrapperPolicy teacher model
2022-04-18 14:49:13,305	INFO	torchdistill.models.util	Frozen module(s): {'model'}
2022-04-18 14:49:13,306	INFO	torchdistill.models.util	[student model]
2022-04-18 14:49:13,306	INFO	torchdistill.models.util	Using the WrapperPolicy student model
2022-04-18 14:49:13,307	INFO	torchdistill.core.distillation	Loss = 1 * PolicyLoss(
  (cross_entropy_loss): CrossEntropyLoss()
  (kldiv_loss): KLDivLoss()
  (linear1): Linear(in_features=128, out_features=16, bias=True)
  (linear2): Linear(in_features=128, out_features=16, bias=True)
  (mse): MSELoss()
)
2022-04-18 14:49:13,307	INFO	torchdistill.core.distillation	Freezing the whole teacher model
2022-04-18 14:49:14,108	INFO	torchdistill.misc.log	Epoch: [0]  [  0/782]  eta: 0:10:24  lr: 0.1  img/s: 524.1631017609085  loss: 9.1046 (9.1046)  time: 0.7984  data: 0.6762  max mem: 285
2022-04-18 14:49:15,808	INFO	torchdistill.misc.log	Epoch: [0]  [100/782]  eta: 0:00:16  lr: 0.1  img/s: 4249.682677389735  loss: 5.4024 (5.6645)  time: 0.0172  data: 0.0001  max mem: 285
2022-04-18 14:49:17,475	INFO	torchdistill.misc.log	Epoch: [0]  [200/782]  eta: 0:00:12  lr: 0.1  img/s: 3925.183599461894  loss: 5.2736 (5.4965)  time: 0.0167  data: 0.0001  max mem: 285
2022-04-18 14:49:19,127	INFO	torchdistill.misc.log	Epoch: [0]  [300/782]  eta: 0:00:09  lr: 0.1  img/s: 3744.2352251963234  loss: 5.1034 (5.3988)  time: 0.0163  data: 0.0001  max mem: 285
2022-04-18 14:49:20,784	INFO	torchdistill.misc.log	Epoch: [0]  [400/782]  eta: 0:00:07  lr: 0.1  img/s: 4136.5219588868  loss: 4.9744 (5.3172)  time: 0.0171  data: 0.0001  max mem: 285
2022-04-18 14:49:22,592	INFO	torchdistill.misc.log	Epoch: [0]  [500/782]  eta: 0:00:05  lr: 0.1  img/s: 3892.6255220417634  loss: 4.9074 (5.2429)  time: 0.0186  data: 0.0001  max mem: 285
2022-04-18 14:49:24,314	INFO	torchdistill.misc.log	Epoch: [0]  [600/782]  eta: 0:00:03  lr: 0.1  img/s: 3477.0981723034674  loss: 4.7689 (5.1796)  time: 0.0181  data: 0.0001  max mem: 285
2022-04-18 14:49:26,194	INFO	torchdistill.misc.log	Epoch: [0]  [700/782]  eta: 0:00:01  lr: 0.1  img/s: 4013.9281057479516  loss: 4.7387 (5.1160)  time: 0.0169  data: 0.0001  max mem: 285
2022-04-18 14:49:27,560	INFO	torchdistill.misc.log	Epoch: [0] Total time: 0:00:14
2022-04-18 14:49:28,039	INFO	torchdistill.misc.log	Validation:  [  0/157]  eta: 0:01:14  acc1: 54.6875 (54.6875)  acc5: 89.0625 (89.0625)  time: 0.4761  data: 0.4598  max mem: 285
2022-04-18 14:49:28,319	INFO	torchdistill.misc.log	Validation:  [100/157]  eta: 0:00:00  acc1: 48.4375 (49.2110)  acc5: 93.7500 (93.5489)  time: 0.0026  data: 0.0001  max mem: 285
2022-04-18 14:49:28,539	INFO	torchdistill.misc.log	Validation: Total time: 0:00:00
2022-04-18 14:49:28,539	INFO	__main__	 * Acc@1 49.2700	Acc@5 93.5600

2022-04-18 14:49:28,539	INFO	__main__	Best top-1 accuracy: 0.0000 -> 49.2700
2022-04-18 14:49:28,539	INFO	__main__	Updating ckpt at None
2022-04-18 14:49:29,091	INFO	torchdistill.misc.log	Epoch: [1]  [  0/782]  eta: 0:07:10  lr: 0.1  img/s: 1822.8426613790386  loss: 4.2834 (4.2834)  time: 0.5505  data: 0.5153  max mem: 285
2022-04-18 14:49:30,811	INFO	torchdistill.misc.log	Epoch: [1]  [100/782]  eta: 0:00:15  lr: 0.1  img/s: 3878.90087278192  loss: 4.5217 (4.5758)  time: 0.0175  data: 0.0001  max mem: 285
2022-04-18 14:49:32,443	INFO	torchdistill.misc.log	Epoch: [1]  [200/782]  eta: 0:00:11  lr: 0.1  img/s: 3861.658337289428  loss: 4.4939 (4.5467)  time: 0.0158  data: 0.0001  max mem: 285
2022-04-18 14:49:34,012	INFO	torchdistill.misc.log	Epoch: [1]  [300/782]  eta: 0:00:08  lr: 0.1  img/s: 4043.9816206932915  loss: 4.3657 (4.5010)  time: 0.0155  data: 0.0001  max mem: 285
2022-04-18 14:49:35,575	INFO	torchdistill.misc.log	Epoch: [1]  [400/782]  eta: 0:00:06  lr: 0.1  img/s: 4071.89272495601  loss: 4.4352 (4.4787)  time: 0.0158  data: 0.0001  max mem: 285
2022-04-18 14:49:37,257	INFO	torchdistill.misc.log	Epoch: [1]  [500/782]  eta: 0:00:04  lr: 0.1  img/s: 3798.435771897552  loss: 4.3457 (4.4622)  time: 0.0170  data: 0.0001  max mem: 285
2022-04-18 14:49:39,000	INFO	torchdistill.misc.log	Epoch: [1]  [600/782]  eta: 0:00:03  lr: 0.1  img/s: 4230.527895101809  loss: 4.2567 (4.4373)  time: 0.0175  data: 0.0001  max mem: 285
2022-04-18 14:49:40,641	INFO	torchdistill.misc.log	Epoch: [1]  [700/782]  eta: 0:00:01  lr: 0.1  img/s: 3945.0855488441134  loss: 4.3072 (4.4164)  time: 0.0156  data: 0.0001  max mem: 285
2022-04-18 14:49:41,965	INFO	torchdistill.misc.log	Epoch: [1] Total time: 0:00:13
2022-04-18 14:49:42,432	INFO	torchdistill.misc.log	Validation:  [  0/157]  eta: 0:01:12  acc1: 56.2500 (56.2500)  acc5: 95.3125 (95.3125)  time: 0.4647  data: 0.4588  max mem: 285
2022-04-18 14:49:42,695	INFO	torchdistill.misc.log	Validation:  [100/157]  eta: 0:00:00  acc1: 54.6875 (56.2191)  acc5: 95.3125 (95.7147)  time: 0.0026  data: 0.0001  max mem: 285
2022-04-18 14:49:42,891	INFO	torchdistill.misc.log	Validation: Total time: 0:00:00
2022-04-18 14:49:42,891	INFO	__main__	 * Acc@1 55.8300	Acc@5 95.7300

2022-04-18 14:49:42,891	INFO	__main__	Best top-1 accuracy: 49.2700 -> 55.8300
2022-04-18 14:49:42,891	INFO	__main__	Updating ckpt at None
2022-04-18 14:49:43,483	INFO	torchdistill.misc.log	Epoch: [2]  [  0/782]  eta: 0:07:41  lr: 0.1  img/s: 1883.3214483663432  loss: 4.1432 (4.1432)  time: 0.5906  data: 0.5564  max mem: 285
2022-04-18 14:49:45,141	INFO	torchdistill.misc.log	Epoch: [2]  [100/782]  eta: 0:00:15  lr: 0.1  img/s: 3997.4305456278294  loss: 4.2132 (4.2254)  time: 0.0161  data: 0.0001  max mem: 285
2022-04-18 14:49:46,815	INFO	torchdistill.misc.log	Epoch: [2]  [200/782]  eta: 0:00:11  lr: 0.1  img/s: 4023.675030728183  loss: 4.1134 (4.1786)  time: 0.0166  data: 0.0001  max mem: 285
2022-04-18 14:49:48,474	INFO	torchdistill.misc.log	Epoch: [2]  [300/782]  eta: 0:00:08  lr: 0.1  img/s: 3837.0396375019654  loss: 4.0897 (4.1584)  time: 0.0164  data: 0.0001  max mem: 285
2022-04-18 14:49:50,053	INFO	torchdistill.misc.log	Epoch: [2]  [400/782]  eta: 0:00:06  lr: 0.1  img/s: 4017.1117130329376  loss: 4.0727 (4.1456)  time: 0.0158  data: 0.0001  max mem: 285
